{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import torch \n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler, FunctionTransformer\n",
    "from transformer_encoder_decoder_qr_model import TransformerEncoderDecoderQRModel\n",
    "\n",
    "import random \n",
    "import os\n",
    "import sys \n",
    "import copy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nhead =  4      # number of heads in the multi-head attention models\n",
    "d_model = 32    # model dimension\n",
    "d_hid = 64      # dimension of the feedforward network model\n",
    "nlayers = 2     # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "dropout = 0.1   # dropout rate\n",
    "\n",
    "input_length  = 4  # Number of data points in src\n",
    "output_length = 1  # Number of data points in tgt\n",
    "weeks_ahead = 1\n",
    "nfeatures  = 1     # Dimension of data - currently only 1d timeseries data\n",
    "batch_size = 8\n",
    "\n",
    "quantiles = [0.010, 0.025, 0.050, 0.100, 0.150, 0.200, 0.250, 0.300, 0.350,\n",
    "             0.400, 0.450, 0.500, 0.550, 0.600, 0.650, 0.700, 0.750, 0.800,\n",
    "             0.850, 0.900, 0.950, 0.975, 0.990]\n",
    "nquantiles = len(quantiles)\n",
    "\n",
    "q_median_ind = quantiles.index(0.5)\n",
    "q_low_ind = quantiles.index(0.025)\n",
    "q_high_ind = quantiles.index(0.975)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ili_data(state):\n",
    "    \n",
    "    if(state=='US'):\n",
    "        ILI_df = pd.read_csv('./data/ILI_national_2002_2024.csv')\n",
    "        ILI_df = ILI_df[['date','year','week','weighted_ili']]\n",
    "    else:\n",
    "        ILI_df = pd.read_csv('./data/ILI_states_2010_2024.csv')\n",
    "        ILI_df = ILI_df[['date','year','week',state]]\n",
    "        ILI_df = ILI_df.rename(columns={state:'weighted_ili'})\n",
    "\n",
    "    ILI_df = ILI_df[ILI_df.week!=53] #ignore week 53 for now\n",
    "    ILI_df['date'] = pd.to_datetime(ILI_df['date'])\n",
    "    ILI_df = ILI_df.set_index('date')\n",
    "    return ILI_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = 'US'\n",
    "ILI_df = load_ili_data(state)\n",
    "\n",
    "start_covid_date = pd.Timestamp('2020-06-28') \n",
    "start_test_date = pd.Timestamp('2022-07-01') \n",
    "\n",
    "train_ind = np.where(ILI_df.index<start_covid_date)[0]\n",
    "covid_ind = np.where((ILI_df.index>=start_covid_date) & (ILI_df.index<start_test_date))[0]\n",
    "test_ind = np.where((ILI_df.index>=start_test_date))[0]\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(ILI_df.loc[ILI_df.iloc[train_ind].index,'weighted_ili'],label='train') \n",
    "plt.plot(ILI_df.loc[ILI_df.iloc[covid_ind].index,'weighted_ili'],label='throw (covid)') \n",
    "plt.plot(ILI_df.loc[ILI_df.iloc[test_ind].index,'weighted_ili'],label='test') \n",
    "plt.ylabel('weighted ILI') \n",
    "plt.xlabel('week') \n",
    "plt.title('ILI - {}'.format(state)) \n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ili = np.expand_dims(ILI_df['weighted_ili'].values,-1)\n",
    "train_ili = ili[train_ind]\n",
    "test_ili = ili[test_ind]\n",
    "\n",
    "test_dates = ILI_df.index[test_ind]\n",
    "\n",
    "scaler = MinMaxScaler() #FunctionTransformer(lambda x: x) \n",
    "scaler.fit(train_ili)\n",
    "train_data = scaler.transform(train_ili)\n",
    "test_data = scaler.transform(test_ili)\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(ts_data, pred, input_length, output_length, weeks_ahead, batch_size,\n",
    "                    shuffle=True, drop_last=True):\n",
    "    \n",
    "    SOS = np.float32(-2) #0\n",
    "    # EOS = np.float32(2) #0\n",
    "\n",
    "    ix_start = 0\n",
    "    if pred is not None:\n",
    "        ix_start = max(0,(len(ts_data)-len(pred))- (input_length+1))\n",
    "        pred = np.expand_dims(np.append(np.repeat(np.nan, len(ts_data)-len(pred)),pred[:,q_median_ind]),-1)\n",
    "        \n",
    "    ix = range(ix_start, len(ts_data) - input_length - output_length - weeks_ahead + 2)\n",
    "    inputs = []\n",
    "    targets = []\n",
    "\n",
    "    for i in ix:\n",
    "        input_sequence = ts_data[i:i+input_length]\n",
    "        if pred is not None:\n",
    "            input_sequence = np.append(input_sequence[(weeks_ahead-1):],pred[(i+input_length+1):(i+input_length+weeks_ahead)])\n",
    "        target_sequence = np.insert(ts_data[i+input_length+weeks_ahead-1:i+input_length+output_length+weeks_ahead-1],0,SOS)\n",
    "        # target_sequence = np.append(np.insert(ts_data[i+input_length+weeks_ahead-1:i+input_length+output_length+weeks_ahead-1],0,SOS),EOS)\n",
    "        inputs.append(torch.from_numpy(input_sequence.astype(np.float32)))\n",
    "        targets.append(torch.from_numpy(target_sequence.astype(np.float32)))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    inputs = torch.stack(inputs)\n",
    "    targets = torch.stack(targets)\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    dataset = TensorDataset(inputs, targets)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    return (data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_loss(q, y, f):\n",
    "    # q: quantile, y: true values, f: predicted quantiles\n",
    "    e = (y - f)\n",
    "    return torch.max(q * e, (q - 1) * e).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wis_loss(q, y, f):\n",
    "\n",
    "    # Generate quantile pairs\n",
    "    n = len(q)\n",
    "    quantile_pairs_ind = [(i, n-i-1) for i in range(n//2)]\n",
    "\n",
    "    # Calculate Interval Scores (IS) for specified pairs of quantiles\n",
    "    wis = 0\n",
    "    for (q_lower_ind, q_upper_ind) in quantile_pairs_ind:\n",
    "        p = q[q_upper_ind] - q[q_lower_ind]\n",
    "        alpha = 1 - p\n",
    "\n",
    "        # Retrieve predictions for the upper and lower quantiles\n",
    "        L = f[:,:,q_lower_ind]\n",
    "        U = f[:,:,q_upper_ind]\n",
    "\n",
    "        # Interval Score calculation\n",
    "        IS = (U - L) + (2 / alpha) * ((L - y) * (y < L) + (y - U) * (y > U))\n",
    "        \n",
    "        # Weight for each interval score, using alpha/2 as described\n",
    "        wis += (alpha / 2) * IS.mean()  # mean of IS over all observations\n",
    "\n",
    "    # Evaluate median accuracy separately if it is a distinct quantile\n",
    "    if 0.5 in q:\n",
    "        median_predictions = f[:,:,q.index(0.5)]\n",
    "        median_error = abs(median_predictions - y).mean()\n",
    "        wis += median_error\n",
    "\n",
    "    K = len(quantile_pairs_ind)\n",
    "    wis = wis/(K+0.5)\n",
    "    return wis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_loop(num_epochs, optimizer, model, min_loss, training_loss, training_loader):\n",
    "\n",
    "    model.train()\n",
    "    best_model = copy.deepcopy(model) \n",
    "    cur_epoch = len(training_loss)\n",
    "    for epoch in range(cur_epoch,cur_epoch+num_epochs):\n",
    "\n",
    "        avg_train_loss = 0\n",
    "        for input, target in training_loader:\n",
    "            input, target = input.to(device), target.to(device)\n",
    "            tgt_input = target[:, :-1] \n",
    "            tgt_output = target[:, 1:] \n",
    "            output = model(input, tgt_input) \n",
    "            optimizer.zero_grad()\n",
    "            loss = sum(quantile_loss(q, tgt_output.reshape(-1), output[:,:,i].view(-1))\n",
    "                            for i, q in enumerate(quantiles))    \n",
    "            # loss = wis_loss(quantiles, tgt_output, output)  \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_train_loss += loss.item() * input.size(0)\n",
    "        avg_train_loss /= len(training_loader.dataset)\n",
    "        training_loss.append(avg_train_loss) \n",
    "        if avg_train_loss < min_loss:\n",
    "            best_model = copy.deepcopy(model) \n",
    "            min_loss = avg_train_loss\n",
    "        print(f'epoch {epoch}: train loss - {round(avg_train_loss, 4)}')\n",
    "\n",
    "    return (best_model, optimizer, min_loss, training_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_pred(data, prev_pred, model, weeks_ahead):\n",
    "\n",
    "    data_loader = get_data_loader(data, prev_pred, input_length, output_length, \n",
    "                                  weeks_ahead, batch_size, shuffle=False, drop_last=False)\n",
    "    pred = []\n",
    "    model.eval()           # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # No gradients needed for validation, reduces memory and computation\n",
    "        avg_loss = 0\n",
    "        for input, target in data_loader:\n",
    "            input, target = input.to(device), target.to(device)\n",
    "            tgt_input = target[:, :-1] \n",
    "            tgt_output = target[:, 1:] \n",
    "            output = model(input, tgt_input) \n",
    "            loss = wis_loss(quantiles, tgt_output, output)       \n",
    "            avg_loss += loss.item() * input.size(0)\n",
    "            # out = output[:,weeks_ahead-1,:].to('cpu').detach().numpy().squeeze()\n",
    "            out = output[:,0,:].to('cpu').detach().numpy().squeeze()\n",
    "            if(len(out.shape)==1):\n",
    "                out = np.expand_dims(out,0) #last batch may contain only one input\n",
    "            pred.append(out)\n",
    "        avg_loss /= len(data_loader.dataset)\n",
    "        print(avg_loss)\n",
    "\n",
    "    pred = np.concatenate(pred, axis=0)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_pred(train_data, test_data, prev_pred_train, prev_pred_test, weeks_ahead):\n",
    "\n",
    "    training_loader   = get_data_loader(train_data, prev_pred_train, \n",
    "                                        input_length, output_length, \n",
    "                                        weeks_ahead, batch_size)\n",
    "\n",
    "    #initializing the model\n",
    "    model = TransformerEncoderDecoderQRModel(nhead, d_model, d_hid, nlayers, \n",
    "                                             input_length, output_length, \n",
    "                                             nfeatures, nquantiles, dropout)\n",
    "    model = model.to(device)\n",
    "\n",
    "    #training\n",
    "    min_loss = sys.maxsize \n",
    "    training_loss = [] \n",
    "\n",
    "    # Training loop\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr = 1e-3) \n",
    "    (model, optimizer, min_loss, training_loss) = \\\n",
    "        run_training_loop(50, optimizer, model, min_loss, training_loss, training_loader)\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr = 1e-4) \n",
    "    (model, optimizer, min_loss, training_loss) = \\\n",
    "        run_training_loop(10, optimizer, model, min_loss, training_loss, training_loader)\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr = 1e-5) \n",
    "    (model, optimizer, min_loss, training_loss) = \\\n",
    "        run_training_loop(10, optimizer, model, min_loss, training_loss, training_loader)\n",
    "\n",
    "    #training progress\n",
    "    plot_training_progress = True\n",
    "    if(plot_training_progress):\n",
    "        plt.figure()\n",
    "        plt.title('Training') \n",
    "        plt.yscale('log') \n",
    "        plt.plot(training_loss, label = 'training') \n",
    "        plt.ylabel('Loss') \n",
    "        plt.xlabel('Epoch') \n",
    "        plt.legend() \n",
    "        plt.show()\n",
    "    \n",
    "    pred_train = get_model_pred(train_data, prev_pred_train, model, weeks_ahead)\n",
    "    pred_test =  get_model_pred(test_data, prev_pred_test, model, weeks_ahead)\n",
    "    return (model, pred_train, pred_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1) \n",
    "torch.manual_seed(1)\n",
    "\n",
    "weeks_ahead1 = 1\n",
    "model1, pred_train1, pred_test1 = train_and_pred(train_data, test_data,\n",
    "                                                 None, None, weeks_ahead1)\n",
    "\n",
    "weeks_ahead2 = 2\n",
    "model2, pred_train2, pred_test2 = train_and_pred(train_data, test_data,\n",
    "                                                 pred_train1, pred_test1, weeks_ahead2)\n",
    "\n",
    "weeks_ahead3 = 3\n",
    "model3, pred_train3, pred_test3 = train_and_pred(train_data, test_data,\n",
    "                                                 pred_train2, pred_test2, weeks_ahead3)\n",
    "\n",
    "weeks_ahead4 = 4\n",
    "model4, pred_train4, pred_test4 = train_and_pred(train_data, test_data,\n",
    "                                                 pred_train3, pred_test3, weeks_ahead4)\n",
    "\n",
    "weeks_ahead5 = 5\n",
    "model5, pred_train5, pred_test5 = train_and_pred(train_data, test_data,\n",
    "                                                 pred_train4, pred_test4, weeks_ahead5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_ili_data_and_pred(pred_test, weeks_ahead):\n",
    "    pred_test_ili = scaler.inverse_transform(pred_test)\n",
    "    #slice test_ili and test_dates to same dates as pred dates\n",
    "    test_ili_slice = test_ili[input_length+(weeks_ahead-1):]\n",
    "    test_ili_slice = test_ili_slice[:len(pred_test_ili)]\n",
    "    test_dates_slice = test_dates[input_length+(weeks_ahead-1):]\n",
    "    test_dates_slice = test_dates_slice[:len(pred_test_ili)]\n",
    "    return (pred_test_ili,test_ili_slice,test_dates_slice)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_ili1, test_ili1, test_dates1 = get_test_ili_data_and_pred(pred_test1, weeks_ahead1)\n",
    "pred_test_ili2, test_ili2, test_dates2 = get_test_ili_data_and_pred(pred_test2, weeks_ahead2)\n",
    "pred_test_ili3, test_ili3, test_dates3 = get_test_ili_data_and_pred(pred_test3, weeks_ahead3)\n",
    "pred_test_ili4, test_ili4, test_dates4 = get_test_ili_data_and_pred(pred_test4, weeks_ahead4)\n",
    "pred_test_ili5, test_ili5, test_dates5 = get_test_ili_data_and_pred(pred_test5, weeks_ahead5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred_test(pred_test_ili, test_ili, test_dates, weeks_ahead):\n",
    "    wis_score_test = np.round(wis_loss(quantiles,test_ili,np.expand_dims(pred_test_ili,1)),3)\n",
    "    print(f'Weighted Interval Score - weeks_ahead={weeks_ahead} (length={len(test_ili)}): {wis_score_test}')\n",
    "\n",
    "    q_median_ind = quantiles.index(0.5)\n",
    "    lowq = 0.025\n",
    "    uppq = 0.975\n",
    "    q_low_ind = quantiles.index(lowq) #quantiles.index(0.25) #\n",
    "    q_high_ind = quantiles.index(uppq) #quantiles.index(0.75) #\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(test_dates,test_ili,'o--',markersize=3,label='data',color='black',alpha=0.75)\n",
    "    plt.plot(test_dates,pred_test_ili[:,q_median_ind], label='pred median',alpha=0.75, color='green')\n",
    "    plt.plot(test_dates,pred_test_ili[:,q_low_ind], label='pred low ({})'.format(lowq),alpha=0.75, color='blue')\n",
    "    plt.plot(test_dates,pred_test_ili[:,q_high_ind], label='pred high ({})'.format(uppq),alpha=0.75, color='red')\n",
    "    plt.xlabel('week')\n",
    "    plt.ylabel('ILI')\n",
    "    plt.title('state={}, horizon={} weeks (WIS={})'.format(state,weeks_ahead,wis_score_test))\n",
    "    plt.legend(loc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pred_test(pred_test_ili1, test_ili1, test_dates1, weeks_ahead1)\n",
    "plot_pred_test(pred_test_ili2, test_ili2, test_dates2, weeks_ahead2)\n",
    "plot_pred_test(pred_test_ili3, test_ili3, test_dates3, weeks_ahead3)\n",
    "plot_pred_test(pred_test_ili4, test_ili4, test_dates4, weeks_ahead4)\n",
    "plot_pred_test(pred_test_ili5, test_ili5, test_dates5, weeks_ahead5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_pred(model, weeks_ahead, test_dates, test_ili, pred_test_ili):\n",
    "    \n",
    "    models_folder = './models'\n",
    "    if not os.path.isdir(models_folder):\n",
    "        os.makedirs(models_folder)\n",
    "    model_file = '{}/model_ili_{}_{}w_horizon.pth'.format(models_folder,state,weeks_ahead)\n",
    "    torch.save({'model_state_dict': model.state_dict()}, model_file)\n",
    "\n",
    "    df1 = pd.DataFrame(index=test_dates, data=test_ili, columns=['ILI'])\n",
    "    df2 = pd.DataFrame(index=test_dates, data=pred_test_ili, columns=quantiles)\n",
    "    df = pd.concat([df1, df2], axis=1)\n",
    "    output_folder = './output'\n",
    "    if not os.path.isdir(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    output_file = '{}/forecasts_ili_{}_{}w_horizon.csv'.format(output_folder,state,weeks_ahead)\n",
    "    df.to_csv(output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_and_pred(model1, weeks_ahead1, test_dates1, test_ili1, pred_test_ili1)\n",
    "save_model_and_pred(model2, weeks_ahead2, test_dates2, test_ili2, pred_test_ili2)\n",
    "save_model_and_pred(model3, weeks_ahead3, test_dates3, test_ili3, pred_test_ili3)\n",
    "save_model_and_pred(model4, weeks_ahead4, test_dates4, test_ili4, pred_test_ili4)\n",
    "save_model_and_pred(model5, weeks_ahead5, test_dates5, test_ili5, pred_test_ili5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
